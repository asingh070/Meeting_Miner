{
  "meeting_id": "M-2025-12-12-01",
  "title": "AstraChat AI Issues Discussion",
  "language_hint": "auto",
  "segments": [
    {
      "speaker": "Meera",
      "start": "00:00:00",
      "end": "00:01:00",
      "text": "Morning everyone! Can you all hear me?"
    },
    {
      "speaker": "Sarah",
      "start": "00:01:01",
      "end": "00:01:30",
      "text": "Loud and clear. I mean, as clear as my coffee-deprived brain allows."
    },
    {
      "speaker": "Jay",
      "start": "00:01:31",
      "end": "00:02:00",
      "text": "Haha same. I’ve had two coffees already and still loading… like AstraChat’s typing icon."
    },
    {
      "speaker": "Jey",
      "start": "00:02:01",
      "end": "00:02:15",
      "text": "Oof, too soon Jay, too soon."
    },
    {
      "speaker": "Abhi",
      "start": "00:02:16",
      "end": "00:02:35",
      "text": "Guys, I literally joined 10 seconds ago and I already feel attacked."
    },
    {
      "speaker": "Meera",
      "start": "00:02:36",
      "end": "00:03:00",
      "text": "Okay okay, team, let’s settle. But yes, mood relatable."
    },
    {
      "speaker": "Meera",
      "start": "00:03:01",
      "end": "00:04:00",
      "text": "So today’s call is mainly about the issues the business is facing with AstraChat AI—response delays, inconsistent answers, and the typing indicator UI glitch. Also, Sarah mentioned she wanted to discuss a side POC idea related to Snowflake Cortex."
    },
    {
      "speaker": "Sarah",
      "start": "00:04:01",
      "end": "00:04:20",
      "text": "Yup, we’ll get to that in a bit."
    },
    {
      "speaker": "Jay",
      "start": "00:04:21",
      "end": "00:05:10",
      "text": "So from the business side, the biggest complaint right now is delay. Users type a query, hit enter, and the bot takes like 7–8 seconds to respond."
    },
    {
      "speaker": "Sarah",
      "start": "00:05:11",
      "end": "00:05:40",
      "text": "And sometimes longer. And we’re getting messages like 'Is it broken?' or 'Is it thinking about its life choices?'"
    },
    {
      "speaker": "Jey",
      "start": "00:05:41",
      "end": "00:06:00",
      "text": "To be fair, Astra is thinking. She’s a deep thinker, okay?"
    },
    {
      "speaker": "Meera",
      "start": "00:06:01",
      "end": "00:06:20",
      "text": "Haha, please. Let’s get serious guys. What do you think is causing the delay?"
    },
    {
      "speaker": "Abhi",
      "start": "00:06:21",
      "end": "00:07:00",
      "text": "From our initial analysis, it’s mainly because of how we’re making multiple API hops. The bot calls our middle layer, which calls the LLM, which then calls your internal knowledge base."
    },
    {
      "speaker": "Jey",
      "start": "00:07:01",
      "end": "00:07:30",
      "text": "And each hop adds latency. We can optimize by implementing caching for repeated queries and reducing some unnecessary validation steps."
    },
    {
      "speaker": "Sarah",
      "start": "00:07:31",
      "end": "00:08:00",
      "text": "Another big one—the answers aren’t always consistent. Like sometimes Astra gives a great response, and the next time for the exact same question, it gives something vague or partially unrelated."
    },
    {
      "speaker": "Jay",
      "start": "00:08:01",
      "end": "00:08:20",
      "text": "Yeah. Users think the bot is moody."
    },
    {
      "speaker": "Abhi",
      "start": "00:08:21",
      "end": "00:08:35",
      "text": "Not moody… just… contextually creative."
    },
    {
      "speaker": "Jey",
      "start": "00:08:36",
      "end": "00:09:10",
      "text": "More specifically, the issue is the LLM is generating responses without enough grounding in your structured knowledge source. Some calls rely too heavily on the LLM’s own reasoning instead of your dataset."
    },
    {
      "speaker": "Meera",
      "start": "00:09:11",
      "end": "00:09:30",
      "text": "What can we do?"
    },
    {
      "speaker": "Abhi",
      "start": "00:09:31",
      "end": "00:10:00",
      "text": "We’re planning to introduce more strict retrieval-augmented generation (RAG) rules. Basically, force the model to rely on the knowledge base before generative output."
    },
    {
      "speaker": "Jey",
      "start": "00:10:01",
      "end": "00:10:25",
      "text": "Also, we can add test prompts to check consistency before pushing changes."
    },
    {
      "speaker": "Sarah",
      "start": "00:10:26",
      "end": "00:10:45",
      "text": "Yes please. We need Astra to be smart, not… unpredictable."
    },
    {
      "speaker": "Meera",
      "start": "00:10:46",
      "end": "00:11:10",
      "text": "And the UI issue—the typing icon disappearing long before the response shows. That’s causing confusion."
    },
    {
      "speaker": "Jay",
      "start": "00:11:11",
      "end": "00:11:30",
      "text": "Yeah customers think the bot has stopped responding."
    },
    {
      "speaker": "Abhi",
      "start": "00:11:31",
      "end": "00:11:50",
      "text": "That’s a frontend timing issue. The current trigger stops the typing animation as soon as the API acknowledges the request—*not* when the response actually arrives."
    },
    {
      "speaker": "Jey",
      "start": "00:11:51",
      "end": "00:12:10",
      "text": "We’ll change the frontend to keep the typing indicator on until the full response payload is received."
    },
    {
      "speaker": "Meera",
      "start": "00:12:11",
      "end": "00:12:25",
      "text": "How long to fix that?"
    },
    {
      "speaker": "Jey",
      "start": "00:12:26",
      "end": "00:12:40",
      "text": "Two days, max."
    },
    {
      "speaker": "Sarah",
      "start": "00:12:41",
      "end": "00:13:20",
      "text": "So while we have you both… we’ve been thinking about another POC. Something to use Snowflake Cortex with a structured data source."
    },
    {
      "speaker": "Jay",
      "start": "00:13:21",
      "end": "00:13:50",
      "text": "The idea is to quickly answer internal queries about sales numbers, product inventory, and maybe forecasting. But we’re not sure what’s feasible."
    },
    {
      "speaker": "Jey",
      "start": "00:13:51",
      "end": "00:14:10",
      "text": "Cortex is great for SQL-backed insights. What exactly do you want it to do?"
    },
    {
      "speaker": "Sarah",
      "start": "00:14:11",
      "end": "00:14:40",
      "text": "Like natural-language queries—'What were last month’s sales for Region X?' or 'Which products are low in stock?'"
    },
    {
      "speaker": "Abhi",
      "start": "00:14:41",
      "end": "00:15:10",
      "text": "That’s totally doable. Cortex can interpret NL queries and convert them to SQL automatically."
    },
    {
      "speaker": "Jay",
      "start": "00:15:11",
      "end": "00:15:30",
      "text": "Oh nice. We weren’t sure if that was really practical or just marketing hype."
    },
    {
      "speaker": "Jey",
      "start": "00:15:31",
      "end": "00:15:50",
      "text": "It’s practical as long as your data models are clean. Cortex hates messy tables."
    },
    {
      "speaker": "Meera",
      "start": "00:15:51",
      "end": "00:16:05",
      "text": "Same as all of us, honestly."
    },
    {
      "speaker": "Abhi",
      "start": "00:16:06",
      "end": "00:16:25",
      "text": "One suggestion—start small. Maybe just build a POC with one dataset—like Sales—and build a few sample prompt templates."
    },
    {
      "speaker": "Sarah",
      "start": "00:16:26",
      "end": "00:16:45",
      "text": "Makes sense. Maybe we’ll schedule a separate session for that."
    },
    {
      "speaker": "Meera",
      "start": "00:16:46",
      "end": "00:17:10",
      "text": "Okay, for AstraChat AI, can you list out what you’ll be working on?"
    },
    {
      "speaker": "Jey",
      "start": "00:17:11",
      "end": "00:17:40",
      "text": "Sure. Optimize latency—caching + streamlining APIs; improve consistency—stronger grounding via RAG and test prompts; fix typing indicator UI—ensure it stays until final response delivery."
    },
    {
      "speaker": "Abhi",
      "start": "00:17:41",
      "end": "00:18:00",
      "text": "And we’ll share a timeline by tomorrow lunchtime."
    },
    {
      "speaker": "Jay",
      "start": "00:18:01",
      "end": "00:18:20",
      "text": "Perfect."
    },
    {
      "speaker": "Sarah",
      "start": "00:18:21",
      "end": "00:18:45",
      "text": "Alright guys, this was good. And thank you for not roasting Astra too much."
    },
    {
      "speaker": "Jey",
      "start": "00:18:46",
      "end": "00:19:05",
      "text": "Don’t worry. We love Astra. She’s like our digital child."
    },
    {
      "speaker": "Abhi",
      "start": "00:19:06",
      "end": "00:19:20",
      "text": "A slightly chaotic child, but still."
    },
    {
      "speaker": "Jay",
      "start": "00:19:21",
      "end": "00:19:40",
      "text": "Same as me when I was young."
    },
    {
      "speaker": "Meera",
      "start": "00:19:41",
      "end": "00:20:00",
      "text": "Okay team. I’ll send the meeting notes. Thanks everyone!"
    },
    {
      "speaker": "All",
      "start": "00:20:01",
      "end": "00:20:10",
      "text": "Thanks! Bye!"
    }
  ]
}